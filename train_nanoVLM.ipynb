{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2de5dd1f",
      "metadata": {
        "id": "2de5dd1f"
      },
      "source": [
        "### Train a VLM in Google Colab!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OCooV08mNANR",
      "metadata": {
        "id": "OCooV08mNANR"
      },
      "source": [
        "### Clone the repository if you don't have it already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ooQMjmrMLn-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooQMjmrMLn-4",
        "outputId": "2a996fad-6645-4a62-db8f-d2d64ade0094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoVLM'...\n",
            "remote: Enumerating objects: 1685, done.\u001b[K\n",
            "remote: Counting objects: 100% (1214/1214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (379/379), done.\u001b[K\n",
            "remote: Total 1685 (delta 902), reused 835 (delta 835), pack-reused 471 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1685/1685), 13.48 MiB | 19.03 MiB/s, done.\n",
            "Resolving deltas: 100% (1168/1168), done.\n",
            "/content/nanoVLM\n",
            "assets\t       generate.py\t      README.md\t\t   train.py\n",
            "data\t       LICENSE\t\t      run_evaluation.py    train.sh\n",
            "eval\t       merge_eval_results.py  slurm\t\t   utils\n",
            "eval.slurm     models\t\t      tests\n",
            "evaluation.py  prepare.sh\t      train_nanoVLM.ipynb\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('nanoVLM'):\n",
        "    !git clone https://github.com/huggingface/nanoVLM.git\n",
        "%cd nanoVLM/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMhc9OCENup5",
      "metadata": {
        "id": "mMhc9OCENup5"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bcw8qQqoOSR7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcw8qQqoOSR7",
        "outputId": "93abed03-ce80-4a65-d54f-1d25fcc0c1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/193.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.5.0 requires fsspec[http]<=2024.12.0,>=2023.1.0, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# If you get an \"Error\" from pip's dependency resolver but the cell completes fine, this is not an issue, you can continue :)\n",
        "!pip -q install torch\n",
        "!pip -q install gcsfs\n",
        "!pip -q install datasets==3.5.0\n",
        "!pip -q install tqdm\n",
        "!pip -q install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "54bc8463",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "bba6939427b74cb2abf12d798619488a",
            "8ed9d15a905744c7bc37fb53417d422b",
            "aea9d9b08cc9414fae8c6945b9ccd49c",
            "31aabfbc932a4b98a336313aad504ef4",
            "00c2c2a0ceb1491b808f469e448389ca",
            "bf3c48335507431ba0ac35b91a534aa1",
            "c434112f7ec643fc82185314b11f4710",
            "31e06f592e554d0aac31e2ead1743e07",
            "aa87f7cae8da4b44b6dea7b389fe4f9e",
            "af14477e16ee47a592911cde403a315a",
            "3a2f8c16b6054abcaf2d743994d017db",
            "1dab6c6f5149421d9f508c67288364c1",
            "bfd3cd0b7f0d4b16963b61796d46f3d1",
            "3a6fe5cce5eb4091be4817f410f652ad",
            "a2edbb0912064011865cf041fcce98ab",
            "7f8e3a26f8b0415abf5cd7ea643e259a",
            "323e17cc8c6a4aff8da4283d73650094",
            "0ccbbbd1b5f2435f986fe4b89f65439c",
            "ce9b4a8a1479418c8c229c5524068218",
            "94ed82aebf6e4f168b4327fea6c34f28"
          ]
        },
        "id": "54bc8463",
        "outputId": "c2f9182d-5cb4-4fb1-b108-06e19953c9c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bba6939427b74cb2abf12d798619488a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's authentificate with the Hugging Face Hub so you can push your model\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5e8dc5ba",
      "metadata": {
        "id": "5e8dc5ba"
      },
      "outputs": [],
      "source": [
        "# Decide on the name of your model here!\n",
        "# You will need your HF user name and the name you want to give to it\n",
        "# For me, this would be \"lusxvr/nanoVLM\"\n",
        "hf_model_name = \"tiltaf/nanoVLM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "OTsl1jZrMeaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTsl1jZrMeaJ",
        "outputId": "3cdbd387-bfa3-4195-e9ba-25bc71540d9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# nanoVLM Imports (please check out the implementations in detail, that's where all the interesting stuff is!)\n",
        "from data.datasets import VQADataset\n",
        "from data.collators import VQACollator\n",
        "from data.data_utils import synchronized_dataloader_step\n",
        "from data.advanced_datasets import ConstantLengthDataset\n",
        "from data.processors import get_image_processor, get_tokenizer\n",
        "\n",
        "import models.config as config\n",
        "from models.vision_language_model import VisionLanguageModel\n",
        "\n",
        "# Libraries\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass, field\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, concatenate_datasets, get_dataset_config_names\n",
        "\n",
        "#Otherwise, the tokenizer will throw a warning\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# To reload the modules if you change something in the code\n",
        "# %reload_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Vzo03IzN3Zf",
      "metadata": {
        "id": "4Vzo03IzN3Zf"
      },
      "source": [
        "### Get the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "3Zzn2FI2N7Aj",
      "metadata": {
        "id": "3Zzn2FI2N7Aj"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(train_cfg, vlm_cfg):\n",
        "    # Create datasets\n",
        "    image_processor = get_image_processor(vlm_cfg.max_img_size, vlm_cfg.vit_img_size, vlm_cfg.resize_to_max_side_len)\n",
        "    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens, vlm_cfg.lm_chat_template)\n",
        "\n",
        "    # Load and combine all training datasets\n",
        "    dataset_names_to_load = train_cfg.train_dataset_name\n",
        "    if \"all\" in dataset_names_to_load:\n",
        "        dataset_names_to_load = get_dataset_config_names(train_cfg.train_dataset_path)\n",
        "\n",
        "    combined_train_data = []\n",
        "\n",
        "    for dataset_name in dataset_names_to_load:\n",
        "        print(f\"Loading dataset: {dataset_name}\")\n",
        "        try:\n",
        "            train_ds = load_dataset(train_cfg.train_dataset_path, dataset_name)['train']\n",
        "            train_ds[0] # Check if the dataset is loaded correctly\n",
        "            combined_train_data.append(train_ds)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to load dataset config '{dataset_name}' from '{train_cfg.train_dataset_path}'. Error: {e}\")\n",
        "            continue\n",
        "    train_ds = concatenate_datasets(combined_train_data)\n",
        "\n",
        "    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatenated datasets\n",
        "\n",
        "    # Apply cutoff if specified\n",
        "    if train_cfg.data_cutoff_idx is None:\n",
        "        total_samples = len(train_ds)  # Use the entire dataset\n",
        "    else:\n",
        "        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n",
        "\n",
        "    val_size = int(total_samples * train_cfg.val_ratio)\n",
        "    train_size = total_samples - val_size\n",
        "\n",
        "    val_ds = train_ds.select(range(train_size, total_samples-1))\n",
        "    train_ds = train_ds.select(range(train_size))\n",
        "\n",
        "    train_dataset = VQADataset(train_ds, tokenizer, image_processor, vlm_cfg.mp_image_token_length)\n",
        "    val_dataset = VQADataset(val_ds, tokenizer, image_processor, vlm_cfg.mp_image_token_length)\n",
        "\n",
        "    train_dataset = ConstantLengthDataset(train_dataset, infinite=False, max_sample_length=train_cfg.max_sample_length, seq_length=vlm_cfg.lm_max_length, num_of_sequences=train_cfg.batch_size*4, queue_size=8,\n",
        "                                        max_images_per_example=train_cfg.max_images_per_example, max_images_per_knapsack=train_cfg.max_images_per_knapsack)\n",
        "\n",
        "    # Create collators\n",
        "    vqa_collator = VQACollator(tokenizer, vlm_cfg.lm_max_length)\n",
        "\n",
        "    # Create dataloaders\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,    # =per device BS in DDP\n",
        "        collate_fn=vqa_collator,\n",
        "        num_workers=1,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=vqa_collator,\n",
        "        num_workers=1,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    # Warmup dataloaders to kickstart worker processes\n",
        "    print(\"Warming up dataloaders...\")\n",
        "    next(iter(train_loader))\n",
        "    next(iter(val_loader))\n",
        "    print(\"Warmup complete.\")\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_F8u3MJ6PAfd",
      "metadata": {
        "id": "_F8u3MJ6PAfd"
      },
      "source": [
        "### Prepare the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "KxOtMU5zPD-4",
      "metadata": {
        "id": "KxOtMU5zPD-4"
      },
      "outputs": [],
      "source": [
        "def get_lr(it, max_lr, max_steps):\n",
        "    min_lr = max_lr * 0.1\n",
        "    warmup_steps = max_steps * 0.03\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "def train(train_cfg, vlm_cfg):\n",
        "    train_loader, val_loader = get_dataloaders(train_cfg, vlm_cfg)\n",
        "\n",
        "    # Initialize model\n",
        "    if train_cfg.resume_from_vlm_checkpoint:\n",
        "        print(f\"Resuming from VLM checkpoint: {vlm_cfg.vlm_checkpoint_path}\")\n",
        "        model = VisionLanguageModel.from_pretrained(vlm_cfg.vlm_checkpoint_path)\n",
        "    else:\n",
        "        model = VisionLanguageModel(vlm_cfg, load_backbone=vlm_cfg.vlm_load_backbone_weights)\n",
        "\n",
        "    print(f\"nanoVLM initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "    print(f\"Training summary: {len(train_loader.dataset)} samples, {len(train_loader)} batches/epoch, batch size {train_cfg.batch_size}\")\n",
        "\n",
        "    # Define optimizer groups\n",
        "    # Since we have pretrained vision and language backbones, but a newly initialized modality projection layer, it doesn't make sense to train them with the same learning rate\n",
        "    # You could opt to fully freeze the backbones and only train the MP layer, but finetuning them with a lower learning rate makes the training as a whole easier\n",
        "    param_groups = []\n",
        "    if train_cfg.lr_mp > 0:\n",
        "        param_groups.append({'params': list(model.MP.parameters()), 'lr': train_cfg.lr_mp})\n",
        "    else:\n",
        "        for p in list(model.MP.parameters()):\n",
        "            p.requires_grad = False\n",
        "    if train_cfg.lr_vision_backbone > 0:\n",
        "        param_groups.append({'params': list(model.vision_encoder.parameters()), 'lr': train_cfg.lr_vision_backbone})\n",
        "    else:\n",
        "        for p in list(model.vision_encoder.parameters()):\n",
        "            p.requires_grad = False\n",
        "    if train_cfg.lr_language_backbone > 0:\n",
        "        param_groups.append({'params': list(model.decoder.parameters()), 'lr': train_cfg.lr_language_backbone})\n",
        "    else:\n",
        "        for p in list(model.decoder.parameters()):\n",
        "            p.requires_grad = False\n",
        "\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "    all_params = [p for group in optimizer.param_groups for p in group['params']]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    epoch_times = []\n",
        "    batch_losses = []\n",
        "    val_losses = []\n",
        "    val_plot_steps = []\n",
        "    global_step = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while global_step < train_cfg.max_training_steps:\n",
        "        epoch_start_time = time.time()\n",
        "        epoch += 1\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_tokens_processed = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(\"Starting training loop\")\n",
        "        for i, batch in enumerate(synchronized_dataloader_step(train_loader, False)):\n",
        "            batch_start_time = time.time()\n",
        "            is_update_step = (i + 1) % train_cfg.gradient_accumulation_steps == 0 or i + 1 == len(train_loader)\n",
        "            images = batch[\"images\"]\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16): # Mixed precision training\n",
        "                _, loss = model(input_ids, images, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            if train_cfg.gradient_accumulation_steps > 1:\n",
        "                loss = loss / train_cfg.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if is_update_step:\n",
        "                if train_cfg.max_grad_norm is not None:\n",
        "                    _ = torch.nn.utils.clip_grad_norm_(all_params, max_norm=train_cfg.max_grad_norm)\n",
        "\n",
        "                param_group_idx = 0\n",
        "                if train_cfg.lr_mp > 0:\n",
        "                    adj_lr_mp = get_lr(global_step, train_cfg.lr_mp, train_cfg.max_training_steps)\n",
        "                    optimizer.param_groups[param_group_idx]['lr'] = adj_lr_mp\n",
        "                    param_group_idx += 1\n",
        "\n",
        "                if train_cfg.lr_vision_backbone > 0:\n",
        "                    adj_lr_vision_backbone = get_lr(global_step, train_cfg.lr_vision_backbone, train_cfg.max_training_steps)\n",
        "                    optimizer.param_groups[param_group_idx]['lr'] = adj_lr_vision_backbone\n",
        "                    param_group_idx += 1\n",
        "\n",
        "                if train_cfg.lr_language_backbone > 0:\n",
        "                    adj_lr_language_backbone = get_lr(global_step, train_cfg.lr_language_backbone, train_cfg.max_training_steps)\n",
        "                    optimizer.param_groups[param_group_idx]['lr'] = adj_lr_language_backbone\n",
        "\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            if train_cfg.gradient_accumulation_steps > 1:\n",
        "                batch_loss = batch_loss * train_cfg.gradient_accumulation_steps\n",
        "            total_train_loss += batch_loss\n",
        "            batch_losses.append(batch_loss)\n",
        "\n",
        "            num_tokens = torch.sum(attention_mask).item() # Sum of attention mask gives number of tokens\n",
        "            total_tokens_processed += num_tokens\n",
        "\n",
        "            batch_end_time = time.time()\n",
        "            batch_duration = batch_end_time - batch_start_time\n",
        "            tokens_per_second = num_tokens / batch_duration\n",
        "\n",
        "            if global_step % 20 == 0:\n",
        "                model.eval()\n",
        "                torch.cuda.empty_cache()  # Clear GPU memory\n",
        "                with torch.no_grad():\n",
        "                    total_val_loss = 0\n",
        "                    for batch in synchronized_dataloader_step(val_loader, False):\n",
        "                        images = batch[\"images\"]\n",
        "                        input_ids = batch[\"input_ids\"].to(device)\n",
        "                        labels = batch[\"labels\"].to(device)\n",
        "                        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "                        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                            _, loss = model(input_ids, images, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "                        total_val_loss += loss.item()\n",
        "                    avg_val_loss = total_val_loss / len(val_loader)\n",
        "                    val_losses.append(avg_val_loss)\n",
        "                    val_plot_steps.append(global_step)\n",
        "                print(f\"\\nStep: {global_step}, Loss: {batch_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Tokens/s: {tokens_per_second:.2f}\")\n",
        "                model.train()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        epoch_tokens_per_second = total_tokens_processed / epoch_duration\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {epoch_duration:.2f}s | T/s: {epoch_tokens_per_second:.2f}\")\n",
        "\n",
        "    model.save_pretrained(save_directory=vlm_cfg.vlm_checkpoint_path)\n",
        "    model.push_to_hub(hf_model_name)\n",
        "\n",
        "    total_training_time = sum(epoch_times)\n",
        "    print(f\"Total training time: {total_training_time:.2f}s\")\n",
        "\n",
        "    plt.plot(batch_losses, label='Train Loss')\n",
        "    plt.plot(val_plot_steps, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4HmsEPNQZbh",
      "metadata": {
        "id": "d4HmsEPNQZbh"
      },
      "source": [
        "### Prepare the Configs\n",
        "Instead of using the config.py file in the repo (which was created to run on one H100), we will create our config here to play around with the parameters easier and adapt them to colabs capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "h8FlqtizQdO-",
      "metadata": {
        "id": "h8FlqtizQdO-"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class VLMConfig:\n",
        "    vit_hidden_dim: int = 768\n",
        "    vit_inter_dim: int = 4 * vit_hidden_dim\n",
        "    vit_patch_size: int = 16\n",
        "    vit_img_size: int = 512\n",
        "    vit_n_heads: int = 12\n",
        "    vit_dropout: float = 0.0\n",
        "    vit_n_blocks: int = 12\n",
        "    vit_ln_eps: float = 1e-6\n",
        "    vit_cls_flag: bool = False\n",
        "    vit_model_type: str = 'google/siglip2-base-patch16-512'\n",
        "\n",
        "    lm_hidden_dim: int = 960\n",
        "    lm_inter_dim: int = 2560\n",
        "    lm_rms_eps: float = 1e-5\n",
        "    lm_re_base: int = 100000\n",
        "    lm_max_position_embeddings: int = 8192\n",
        "    lm_base_vocab_size: int = 49152\n",
        "    extra_token_amount: int = 66  # Number of extra tokens for the VLM (image start, image end, image token)\n",
        "    lm_vocab_size: int = lm_base_vocab_size + extra_token_amount # Not a great way to do this, but it works for now (vlm_extra_tokens cannot be a dict, since this is mutable, and a Field has no len() function)\n",
        "    lm_n_heads: int = 15\n",
        "    lm_n_kv_heads: int = 5\n",
        "    lm_dropout: float = 0.0\n",
        "    lm_n_blocks: int = 32\n",
        "    lm_attn_scaling: float = 1.0\n",
        "    lm_max_length: int = 256\n",
        "    lm_use_tokens: bool = False # Decide if the LM expects tokens or embeddings as input (if using as a backbone for the VLM, set to False)\n",
        "    lm_tie_weights: bool = True # Decide if you want to tie the LM Head weight to the token embedding weights\n",
        "    lm_model_type: str = 'HuggingFaceTB/SmolLM2-135M'\n",
        "    lm_tokenizer: str = 'HuggingFaceTB/SmolLM2-360M-Instruct'\n",
        "    lm_chat_template: str = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
        "\n",
        "    mp_pixel_shuffle_factor: int = 4\n",
        "    mp_image_token_length: int = 64\n",
        "\n",
        "    max_img_size: int = 512\n",
        "    resize_to_max_side_len: bool = False\n",
        "\n",
        "    vlm_extra_tokens: dict[str, str] = field(default_factory=lambda: {\"image_token\": \"<|image|>\", \"global_image_token\": \"<|global_image|>\",\n",
        "      \"r1c1\": \"<row_1_col_1>\", \"r1c2\": \"<row_1_col_2>\", \"r1c3\": \"<row_1_col_3>\", \"r1c4\": \"<row_1_col_4>\", \"r1c5\": \"<row_1_col_5>\", \"r1c6\": \"<row_1_col_6>\", \"r1c7\": \"<row_1_col_7>\", \"r1c8\": \"<row_1_col_8>\",\n",
        "      \"r2c1\": \"<row_2_col_1>\", \"r2c2\": \"<row_2_col_2>\", \"r2c3\": \"<row_2_col_3>\", \"r2c4\": \"<row_2_col_4>\", \"r2c5\": \"<row_2_col_5>\", \"r2c6\": \"<row_2_col_6>\", \"r2c7\": \"<row_2_col_7>\", \"r2c8\": \"<row_2_col_8>\",\n",
        "      \"r3c1\": \"<row_3_col_1>\", \"r3c2\": \"<row_3_col_2>\", \"r3c3\": \"<row_3_col_3>\", \"r3c4\": \"<row_3_col_4>\", \"r3c5\": \"<row_3_col_5>\", \"r3c6\": \"<row_3_col_6>\", \"r3c7\": \"<row_3_col_7>\", \"r3c8\": \"<row_3_col_8>\",\n",
        "      \"r4c1\": \"<row_4_col_1>\", \"r4c2\": \"<row_4_col_2>\", \"r4c3\": \"<row_4_col_3>\", \"r4c4\": \"<row_4_col_4>\", \"r4c5\": \"<row_4_col_5>\", \"r4c6\": \"<row_4_col_6>\", \"r4c7\": \"<row_4_col_7>\", \"r4c8\": \"<row_4_col_8>\",\n",
        "      \"r5c1\": \"<row_5_col_1>\", \"r5c2\": \"<row_5_col_2>\", \"r5c3\": \"<row_5_col_3>\", \"r5c4\": \"<row_5_col_4>\", \"r5c5\": \"<row_5_col_5>\", \"r5c6\": \"<row_5_col_6>\", \"r5c7\": \"<row_5_col_7>\", \"r5c8\": \"<row_5_col_8>\",\n",
        "      \"r6c1\": \"<row_6_col_1>\", \"r6c2\": \"<row_6_col_2>\", \"r6c3\": \"<row_6_col_3>\", \"r6c4\": \"<row_6_col_4>\", \"r6c5\": \"<row_6_col_5>\", \"r6c6\": \"<row_6_col_6>\", \"r6c7\": \"<row_6_col_7>\", \"r6c8\": \"<row_6_col_8>\",\n",
        "      \"r7c1\": \"<row_7_col_1>\", \"r7c2\": \"<row_7_col_2>\", \"r7c3\": \"<row_7_col_3>\", \"r7c4\": \"<row_7_col_4>\", \"r7c5\": \"<row_7_col_5>\", \"r7c6\": \"<row_7_col_6>\", \"r7c7\": \"<row_7_col_7>\", \"r7c8\": \"<row_7_col_8>\",\n",
        "      \"r8c1\": \"<row_8_col_1>\", \"r8c2\": \"<row_8_col_2>\", \"r8c3\": \"<row_8_col_3>\", \"r8c4\": \"<row_8_col_4>\", \"r8c5\": \"<row_8_col_5>\", \"r8c6\": \"<row_8_col_6>\", \"r8c7\": \"<row_8_col_7>\", \"r8c8\": \"<row_8_col_8>\"})\n",
        "    vlm_load_backbone_weights: bool = True\n",
        "    vlm_checkpoint_path: str = 'checkpoints'\n",
        "    hf_repo_name: str = 'nanoVLM'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    lr_mp: float = 0.005\n",
        "    lr_vision_backbone: float = 0.0005\n",
        "    lr_language_backbone: float = 0.0005\n",
        "    data_cutoff_idx: int = 128 # Let's only use a small subset at first\n",
        "    val_ratio: float = 0.2\n",
        "    batch_size: int = 1\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    max_grad_norm: float = 1.0\n",
        "    max_training_steps: int = 200\n",
        "    max_images_per_example: int = 2\n",
        "    max_images_per_knapsack: int = 8\n",
        "    max_sample_length: int = 256\n",
        "    compile: bool = False\n",
        "    resume_from_vlm_checkpoint: bool = False # Indicate if the training should be resumed from a checkpoint of the whole VLM or you want to start from scratch\n",
        "    train_dataset_path: str = 'HuggingFaceM4/the_cauldron'\n",
        "    train_dataset_name: tuple[str, ...] = (\"tqa\", ) #All options; (\"ai2d\", \"aokvqa\", \"chart2text\", \"chartqa\", \"clevr\", \"cocoqa\", \"datikz\", \"diagram_image_to_text\", \"docvqa\", \"dvqa\", \"figureqa\", \"finqa\", \"geomverse\", \"hateful_memes\", \"hitab\", \"iam\", \"iconqa\", \"infographic_vqa\", \"intergps\", \"localized_narratives\", \"mapqa\", \"multihiertt\", \"ocrvqa\", \"plotqa\", \"raven\", \"rendered_text\", \"robut_sqa\", \"robut_wikisql\", \"robut_wtq\", \"scienceqa\", \"screen2words\", \"st_vqa\", \"tabmwp\", \"tallyqa\", \"tat_qa\", \"textcaps\", \"textvqa\", \"tqa\", \"vistext\", \"visual7w\", \"visualmrc\", \"vqarad\", \"vqav2\", \"vsr\", \"websight\") # \"clevr_math\", \"okvqa\", \"spot_the_diff\", \"nlvr2\", \"mimic_cgd\","
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmFQwKGcSLr_",
      "metadata": {
        "id": "KmFQwKGcSLr_"
      },
      "source": [
        "### Lets run the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "9MlFpXQFSNdx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9MlFpXQFSNdx",
        "outputId": "68368508-ed86-4c02-cdc9-c16c50c6d218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resize to max side len: False\n",
            "Loading dataset: tqa\n",
            "Warming up dataloaders...\n",
            "Warmup complete.\n",
            "Loading from backbone weights\n",
            "Successfully loaded google/siglip2-base-patch16-512 weights from safetensors. Model has 86,433,024 parameters.\n",
            "Extending token embeddings from torch.Size([49152, 576]) to torch.Size([49218, 576])\n",
            "Initialized 66 new token embeddings\n",
            "Successfully loaded HuggingFaceTB/SmolLM2-135M weights from safetensors. Model has 134,553,024 parameters.\n",
            "nanoVLM initialized with 228,063,936 parameters\n",
            "Training summary: 105 samples, 105 batches/epoch, batch size 1\n",
            "Starting training loop\n",
            "\n",
            "Step: 0, Loss: 4.3685, Val Loss: 0.9565, Tokens/s: 658.55\n",
            "\n",
            "Step: 20, Loss: 0.7764, Val Loss: 0.1909, Tokens/s: 786.41\n",
            "Epoch 1 | Train Loss: 0.6489 | Val Loss: 0.1909 | Time: 9.77s | T/s: 598.51\n",
            "Starting training loop\n",
            "\n",
            "Step: 40, Loss: 0.4744, Val Loss: 0.0788, Tokens/s: 751.67\n",
            "Epoch 2 | Train Loss: 0.1206 | Val Loss: 0.0788 | Time: 9.23s | T/s: 634.07\n",
            "Starting training loop\n",
            "\n",
            "Step: 60, Loss: 0.4178, Val Loss: 0.0787, Tokens/s: 1055.44\n",
            "\n",
            "Step: 80, Loss: 0.2488, Val Loss: 0.0816, Tokens/s: 664.81\n",
            "Epoch 3 | Train Loss: 0.0935 | Val Loss: 0.0816 | Time: 10.28s | T/s: 569.17\n",
            "Starting training loop\n",
            "\n",
            "Step: 100, Loss: 0.2545, Val Loss: 0.1043, Tokens/s: 1140.29\n",
            "Epoch 4 | Train Loss: 0.0713 | Val Loss: 0.1043 | Time: 8.65s | T/s: 676.14\n",
            "Starting training loop\n",
            "\n",
            "Step: 120, Loss: 0.2626, Val Loss: 0.0707, Tokens/s: 813.81\n",
            "\n",
            "Step: 140, Loss: 0.0116, Val Loss: 0.0822, Tokens/s: 1066.79\n",
            "Epoch 5 | Train Loss: 0.0542 | Val Loss: 0.0822 | Time: 9.69s | T/s: 604.02\n",
            "Starting training loop\n",
            "\n",
            "Step: 160, Loss: 0.0912, Val Loss: 0.1236, Tokens/s: 567.44\n",
            "Epoch 6 | Train Loss: 0.0284 | Val Loss: 0.1236 | Time: 8.73s | T/s: 670.38\n",
            "Starting training loop\n",
            "\n",
            "Step: 180, Loss: 0.0023, Val Loss: 0.1356, Tokens/s: 848.25\n",
            "\n",
            "Step: 200, Loss: 0.0459, Val Loss: 0.1633, Tokens/s: 1372.35\n",
            "Epoch 7 | Train Loss: 0.0155 | Val Loss: 0.1633 | Time: 10.57s | T/s: 553.50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HfHubHTTPError",
          "evalue": "(Request ID: Root=1-6965fc7b-6871197369d9bc345e6dac9f;4764913e-7a88-4178-a798-7760fd70883d)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"tahir_iltaf1\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/api/repos/create",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-196005043.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvlm_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVLMConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_cfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvlm_cfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3901988680.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_cfg, vlm_cfg)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvlm_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvlm_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhf_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mtotal_training_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_times\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/nanoVLM/models/vision_language_model.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_id, private)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Create repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mrepo_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mrepo_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepo_url\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Created repo: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3771\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mRepoUrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.endpoint}/{repo_type}/{repo_id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3772\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3773\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3774\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3775\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3759\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3760\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3761\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3762\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nMake sure your token has the correct permissions.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             )\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m416\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: (Request ID: Root=1-6965fc7b-6871197369d9bc345e6dac9f;4764913e-7a88-4178-a798-7760fd70883d)\n\n403 Forbidden: You don't have the rights to create a model under the namespace \"tahir_iltaf1\".\nCannot access content at: https://huggingface.co/api/repos/create.\nMake sure your token has the correct permissions."
          ]
        }
      ],
      "source": [
        "vlm_cfg = VLMConfig()\n",
        "train_cfg = TrainConfig()\n",
        "train(train_cfg, vlm_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78d938dd",
      "metadata": {
        "id": "78d938dd"
      },
      "source": [
        "As you can see the model trains, so feel free to play around with the architecture or data! Let us know what you build with it!\n",
        "\n",
        "PS: If you want to test the model, check out generate.py to see how to do inference with it"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nanovlm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bba6939427b74cb2abf12d798619488a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_c434112f7ec643fc82185314b11f4710"
          }
        },
        "8ed9d15a905744c7bc37fb53417d422b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31e06f592e554d0aac31e2ead1743e07",
            "placeholder": "​",
            "style": "IPY_MODEL_aa87f7cae8da4b44b6dea7b389fe4f9e",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "aea9d9b08cc9414fae8c6945b9ccd49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_af14477e16ee47a592911cde403a315a",
            "placeholder": "​",
            "style": "IPY_MODEL_3a2f8c16b6054abcaf2d743994d017db",
            "value": ""
          }
        },
        "31aabfbc932a4b98a336313aad504ef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_1dab6c6f5149421d9f508c67288364c1",
            "style": "IPY_MODEL_bfd3cd0b7f0d4b16963b61796d46f3d1",
            "value": true
          }
        },
        "00c2c2a0ceb1491b808f469e448389ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_3a6fe5cce5eb4091be4817f410f652ad",
            "style": "IPY_MODEL_a2edbb0912064011865cf041fcce98ab",
            "tooltip": ""
          }
        },
        "bf3c48335507431ba0ac35b91a534aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f8e3a26f8b0415abf5cd7ea643e259a",
            "placeholder": "​",
            "style": "IPY_MODEL_323e17cc8c6a4aff8da4283d73650094",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "c434112f7ec643fc82185314b11f4710": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "31e06f592e554d0aac31e2ead1743e07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa87f7cae8da4b44b6dea7b389fe4f9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af14477e16ee47a592911cde403a315a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a2f8c16b6054abcaf2d743994d017db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1dab6c6f5149421d9f508c67288364c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfd3cd0b7f0d4b16963b61796d46f3d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a6fe5cce5eb4091be4817f410f652ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2edbb0912064011865cf041fcce98ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "7f8e3a26f8b0415abf5cd7ea643e259a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "323e17cc8c6a4aff8da4283d73650094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ccbbbd1b5f2435f986fe4b89f65439c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce9b4a8a1479418c8c229c5524068218",
            "placeholder": "​",
            "style": "IPY_MODEL_94ed82aebf6e4f168b4327fea6c34f28",
            "value": "Connecting..."
          }
        },
        "ce9b4a8a1479418c8c229c5524068218": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ed82aebf6e4f168b4327fea6c34f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}