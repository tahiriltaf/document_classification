{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2de5dd1f",
      "metadata": {
        "id": "2de5dd1f"
      },
      "source": [
        "### Train a VLM in Google Colab!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OCooV08mNANR",
      "metadata": {
        "id": "OCooV08mNANR"
      },
      "source": [
        "### Clone the repository if you don't have it already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ooQMjmrMLn-4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooQMjmrMLn-4",
        "outputId": "0da4b7bf-45cf-4b2c-eb5a-b8b8b6b487af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoVLM'...\n",
            "remote: Enumerating objects: 1685, done.\u001b[K\n",
            "remote: Counting objects: 100% (1214/1214), done.\u001b[K\n",
            "remote: Compressing objects: 100% (379/379), done.\u001b[K\n",
            "remote: Total 1685 (delta 902), reused 835 (delta 835), pack-reused 471 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1685/1685), 13.48 MiB | 10.47 MiB/s, done.\n",
            "Resolving deltas: 100% (1168/1168), done.\n",
            "/content/nanoVLM\n",
            "assets\t       generate.py\t      README.md\t\t   train.py\n",
            "data\t       LICENSE\t\t      run_evaluation.py    train.sh\n",
            "eval\t       merge_eval_results.py  slurm\t\t   utils\n",
            "eval.slurm     models\t\t      tests\n",
            "evaluation.py  prepare.sh\t      train_nanoVLM.ipynb\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.isdir('nanoVLM'):\n",
        "    !git clone https://github.com/huggingface/nanoVLM.git\n",
        "%cd nanoVLM/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mMhc9OCENup5",
      "metadata": {
        "id": "mMhc9OCENup5"
      },
      "source": [
        "### Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "bcw8qQqoOSR7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcw8qQqoOSR7",
        "outputId": "7980ca41-dfea-461d-b531-677dac4d8e0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/183.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# If you get an \"Error\" from pip's dependency resolver but the cell completes fine, this is not an issue, you can continue :)\n",
        "!pip -q install torch\n",
        "!pip -q install gcsfs\n",
        "!pip -q install datasets==3.5.0\n",
        "!pip -q install tqdm\n",
        "!pip -q install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "54bc8463",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "86acf5c0d6e741a6891fce588c27b2b9",
            "b1babd82dee64d12a569c68aa5fcb48e",
            "e58f56b20c5b41428e99f87e74ed0db4",
            "70948fe6d5f749708401f1b7a93aece3",
            "1ba21cc831fb42529e59141354da52a0",
            "8c996cb60d4b4af08354cf7614cc0c5e",
            "ec525e316c2142d1ac233cb70b59e2ed",
            "c608e286fe1a4a3797724a909dd365b9",
            "30c9f4edfddb4e8982009868024b07f7",
            "25c753a85f0e4cafaa3ce23f488ddfdd",
            "fe5d0be9f46c4c8f99c2ccbed64b5868",
            "e95f40bd09774f2f9e60c9791f37e3df",
            "96fe816cfca54de7a240c31cc3900ca0",
            "0694f0bd52c641d08e3b8c1085821245",
            "c20a589cd133478ba9cb838b550bfc14",
            "8ee2c4891d704c08a9776b7344f6c148",
            "fb44e5f2903843b0a76ce236d783a603",
            "3c704b0888c64ce8ac2dd387187c7e26",
            "1bb5dc2d759f49d6abc855e97fa4a9e9",
            "dab181be12644606a299592a73f26ca2"
          ]
        },
        "id": "54bc8463",
        "outputId": "958de462-a355-4acf-98ef-039608090423"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86acf5c0d6e741a6891fce588c27b2b9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's authentificate with the Hugging Face Hub so you can push your model\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "5e8dc5ba",
      "metadata": {
        "id": "5e8dc5ba"
      },
      "outputs": [],
      "source": [
        "# Decide on the name of your model here!\n",
        "# You will need your HF user name and the name you want to give to it\n",
        "# For me, this would be \"lusxvr/nanoVLM\"\n",
        "hf_model_name = \"tiltaf/nanoVLM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "OTsl1jZrMeaJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTsl1jZrMeaJ",
        "outputId": "60f126fb-e52e-41f4-90b1-cb56a67040bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# nanoVLM Imports (please check out the implementations in detail, that's where all the interesting stuff is!)\n",
        "from data.datasets import VQADataset\n",
        "from data.collators import VQACollator\n",
        "from data.data_utils import synchronized_dataloader_step\n",
        "from data.advanced_datasets import ConstantLengthDataset\n",
        "from data.processors import get_image_processor, get_tokenizer\n",
        "\n",
        "import models.config as config\n",
        "from models.vision_language_model import VisionLanguageModel\n",
        "\n",
        "# Libraries\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from dataclasses import dataclass, field\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, concatenate_datasets, get_dataset_config_names\n",
        "\n",
        "#Otherwise, the tokenizer will throw a warning\n",
        "import os\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "\n",
        "# To reload the modules if you change something in the code\n",
        "# %reload_ext autoreload\n",
        "# %autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4Vzo03IzN3Zf",
      "metadata": {
        "id": "4Vzo03IzN3Zf"
      },
      "source": [
        "### Get the dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3Zzn2FI2N7Aj",
      "metadata": {
        "id": "3Zzn2FI2N7Aj"
      },
      "outputs": [],
      "source": [
        "def get_dataloaders(train_cfg, vlm_cfg):\n",
        "    # Create datasets\n",
        "    image_processor = get_image_processor(vlm_cfg.max_img_size, vlm_cfg.vit_img_size, vlm_cfg.resize_to_max_side_len)\n",
        "    tokenizer = get_tokenizer(vlm_cfg.lm_tokenizer, vlm_cfg.vlm_extra_tokens, vlm_cfg.lm_chat_template)\n",
        "\n",
        "    # Load and combine all training datasets\n",
        "    dataset_names_to_load = train_cfg.train_dataset_name\n",
        "    if \"all\" in dataset_names_to_load:\n",
        "        dataset_names_to_load = get_dataset_config_names(train_cfg.train_dataset_path)\n",
        "\n",
        "    combined_train_data = []\n",
        "\n",
        "    for dataset_name in dataset_names_to_load:\n",
        "        print(f\"Loading dataset: {dataset_name}\")\n",
        "        try:\n",
        "            train_ds = load_dataset(train_cfg.train_dataset_path, dataset_name)['train']\n",
        "            train_ds[0] # Check if the dataset is loaded correctly\n",
        "            combined_train_data.append(train_ds)\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Failed to load dataset config '{dataset_name}' from '{train_cfg.train_dataset_path}'. Error: {e}\")\n",
        "            continue\n",
        "    train_ds = concatenate_datasets(combined_train_data)\n",
        "\n",
        "    train_ds = train_ds.shuffle(seed=0) # Shuffle the training dataset, so train and val get equal contributions from all concatenated datasets\n",
        "\n",
        "    # Apply cutoff if specified\n",
        "    if train_cfg.data_cutoff_idx is None:\n",
        "        total_samples = len(train_ds)  # Use the entire dataset\n",
        "    else:\n",
        "        total_samples = min(len(train_ds), train_cfg.data_cutoff_idx)\n",
        "\n",
        "    val_size = int(total_samples * train_cfg.val_ratio)\n",
        "    train_size = total_samples - val_size\n",
        "\n",
        "    val_ds = train_ds.select(range(train_size, total_samples-1))\n",
        "    train_ds = train_ds.select(range(train_size))\n",
        "\n",
        "    train_dataset = VQADataset(train_ds, tokenizer, image_processor, vlm_cfg.mp_image_token_length)\n",
        "    val_dataset = VQADataset(val_ds, tokenizer, image_processor, vlm_cfg.mp_image_token_length)\n",
        "\n",
        "    train_dataset = ConstantLengthDataset(train_dataset, infinite=False, max_sample_length=train_cfg.max_sample_length, seq_length=vlm_cfg.lm_max_length, num_of_sequences=train_cfg.batch_size*4, queue_size=8,\n",
        "                                        max_images_per_example=train_cfg.max_images_per_example, max_images_per_knapsack=train_cfg.max_images_per_knapsack)\n",
        "\n",
        "    # Create collators\n",
        "    vqa_collator = VQACollator(tokenizer, vlm_cfg.lm_max_length)\n",
        "\n",
        "    # Create dataloaders\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=train_cfg.batch_size,    # =per device BS in DDP\n",
        "        collate_fn=vqa_collator,\n",
        "        num_workers=1,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=train_cfg.batch_size,\n",
        "        shuffle=False,\n",
        "        collate_fn=vqa_collator,\n",
        "        num_workers=1,\n",
        "        pin_memory=True,\n",
        "        persistent_workers=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    # Warmup dataloaders to kickstart worker processes\n",
        "    print(\"Warming up dataloaders...\")\n",
        "    next(iter(train_loader))\n",
        "    next(iter(val_loader))\n",
        "    print(\"Warmup complete.\")\n",
        "\n",
        "    return train_loader, val_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_F8u3MJ6PAfd",
      "metadata": {
        "id": "_F8u3MJ6PAfd"
      },
      "source": [
        "### Prepare the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "KxOtMU5zPD-4",
      "metadata": {
        "id": "KxOtMU5zPD-4"
      },
      "outputs": [],
      "source": [
        "def get_lr(it, max_lr, max_steps):\n",
        "    min_lr = max_lr * 0.1\n",
        "    warmup_steps = max_steps * 0.03\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "def train(train_cfg, vlm_cfg):\n",
        "    train_loader, val_loader = get_dataloaders(train_cfg, vlm_cfg)\n",
        "\n",
        "    # Initialize model\n",
        "    if train_cfg.resume_from_vlm_checkpoint:\n",
        "        print(f\"Resuming from VLM checkpoint: {vlm_cfg.vlm_checkpoint_path}\")\n",
        "        model = VisionLanguageModel.from_pretrained(vlm_cfg.vlm_checkpoint_path)\n",
        "    else:\n",
        "        model = VisionLanguageModel(vlm_cfg, load_backbone=vlm_cfg.vlm_load_backbone_weights)\n",
        "\n",
        "    print(f\"nanoVLM initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "    print(f\"Training summary: {len(train_loader.dataset)} samples, {len(train_loader)} batches/epoch, batch size {train_cfg.batch_size}\")\n",
        "\n",
        "    # Define optimizer groups\n",
        "    # Since we have pretrained vision and language backbones, but a newly initialized modality projection layer, it doesn't make sense to train them with the same learning rate\n",
        "    # You could opt to fully freeze the backbones and only train the MP layer, but finetuning them with a lower learning rate makes the training as a whole easier\n",
        "    param_groups = []\n",
        "    if train_cfg.lr_mp > 0:\n",
        "        param_groups.append({'params': list(model.MP.parameters()), 'lr': train_cfg.lr_mp})\n",
        "    else:\n",
        "        for p in list(model.MP.parameters()):\n",
        "            p.requires_grad = False\n",
        "    if train_cfg.lr_vision_backbone > 0:\n",
        "        param_groups.append({'params': list(model.vision_encoder.parameters()), 'lr': train_cfg.lr_vision_backbone})\n",
        "    else:\n",
        "        for p in list(model.vision_encoder.parameters()):\n",
        "            p.requires_grad = False\n",
        "    if train_cfg.lr_language_backbone > 0:\n",
        "        param_groups.append({'params': list(model.decoder.parameters()), 'lr': train_cfg.lr_language_backbone})\n",
        "    else:\n",
        "        for p in list(model.decoder.parameters()):\n",
        "            p.requires_grad = False\n",
        "\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "    all_params = [p for group in optimizer.param_groups for p in group['params']]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    epoch_times = []\n",
        "    batch_losses = []\n",
        "    val_losses = []\n",
        "    val_plot_steps = []\n",
        "    global_step = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while global_step < train_cfg.max_training_steps:\n",
        "        epoch_start_time = time.time()\n",
        "        epoch += 1\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "        total_tokens_processed = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(\"Starting training loop\")\n",
        "        for i, batch in enumerate(synchronized_dataloader_step(train_loader, False)):\n",
        "            batch_start_time = time.time()\n",
        "            is_update_step = (i + 1) % train_cfg.gradient_accumulation_steps == 0 or i + 1 == len(train_loader)\n",
        "            images = batch[\"images\"]\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16): # Mixed precision training\n",
        "                _, loss = model(input_ids, images, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            if train_cfg.gradient_accumulation_steps > 1:\n",
        "                loss = loss / train_cfg.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if is_update_step:\n",
        "                if train_cfg.max_grad_norm is not None:\n",
        "                    _ = torch.nn.utils.clip_grad_norm_(all_params, max_norm=train_cfg.max_grad_norm)\n",
        "\n",
        "                param_group_idx = 0\n",
        "                if train_cfg.lr_mp > 0:\n",
        "                    adj_lr_mp = get_lr(global_step, train_cfg.lr_mp, train_cfg.max_training_steps)\n",
        "                    optimizer.param_groups[param_group_idx]['lr'] = adj_lr_mp\n",
        "                    param_group_idx += 1\n",
        "\n",
        "                if train_cfg.lr_vision_backbone > 0:\n",
        "                    adj_lr_vision_backbone = get_lr(global_step, train_cfg.lr_vision_backbone, train_cfg.max_training_steps)\n",
        "                    optimizer.param_groups[param_group_idx]['lr'] = adj_lr_vision_backbone\n",
        "                    param_group_idx += 1\n",
        "\n",
        "                if train_cfg.lr_language_backbone > 0:\n",
        "                    adj_lr_language_backbone = get_lr(global_step, train_cfg.lr_language_backbone, train_cfg.max_training_steps)\n",
        "                    optimizer.param_groups[param_group_idx]['lr'] = adj_lr_language_backbone\n",
        "\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "            if train_cfg.gradient_accumulation_steps > 1:\n",
        "                batch_loss = batch_loss * train_cfg.gradient_accumulation_steps\n",
        "            total_train_loss += batch_loss\n",
        "            batch_losses.append(batch_loss)\n",
        "\n",
        "            num_tokens = torch.sum(attention_mask).item() # Sum of attention mask gives number of tokens\n",
        "            total_tokens_processed += num_tokens\n",
        "\n",
        "            batch_end_time = time.time()\n",
        "            batch_duration = batch_end_time - batch_start_time\n",
        "            tokens_per_second = num_tokens / batch_duration\n",
        "\n",
        "            if global_step % 20 == 0:\n",
        "                model.eval()\n",
        "                torch.cuda.empty_cache()  # Clear GPU memory\n",
        "                with torch.no_grad():\n",
        "                    total_val_loss = 0\n",
        "                    for batch in synchronized_dataloader_step(val_loader, False):\n",
        "                        images = batch[\"images\"]\n",
        "                        input_ids = batch[\"input_ids\"].to(device)\n",
        "                        labels = batch[\"labels\"].to(device)\n",
        "                        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "                        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                            _, loss = model(input_ids, images, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "                        total_val_loss += loss.item()\n",
        "                    avg_val_loss = total_val_loss / len(val_loader)\n",
        "                    val_losses.append(avg_val_loss)\n",
        "                    val_plot_steps.append(global_step)\n",
        "                print(f\"\\nStep: {global_step}, Loss: {batch_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Tokens/s: {tokens_per_second:.2f}\")\n",
        "                model.train()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        epoch_end_time = time.time()\n",
        "        epoch_duration = epoch_end_time - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        epoch_tokens_per_second = total_tokens_processed / epoch_duration\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {epoch_duration:.2f}s | T/s: {epoch_tokens_per_second:.2f}\")\n",
        "\n",
        "    model.save_pretrained(save_directory=vlm_cfg.vlm_checkpoint_path)\n",
        "    model.push_to_hub(hf_model_name)\n",
        "\n",
        "    total_training_time = sum(epoch_times)\n",
        "    print(f\"Total training time: {total_training_time:.2f}s\")\n",
        "\n",
        "    plt.plot(batch_losses, label='Train Loss')\n",
        "    plt.plot(val_plot_steps, val_losses, label='Val Loss')\n",
        "    plt.xlabel('Batch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Loss Curve')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# LR schedule (unchanged logic)\n",
        "# ----------------------------\n",
        "def get_lr(it, max_lr, max_steps):\n",
        "    min_lr = max_lr * 0.1\n",
        "    warmup_steps = int(max_steps * 0.03)\n",
        "\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it + 1) / warmup_steps\n",
        "\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    decay_ratio = float(np.clip(decay_ratio, 0.0, 1.0))\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Pretty training dashboard\n",
        "# ----------------------------\n",
        "def _moving_average(x: np.ndarray, window: int) -> np.ndarray:\n",
        "    if window <= 1 or x.size == 0:\n",
        "        return x\n",
        "    window = min(window, x.size)\n",
        "    kernel = np.ones(window, dtype=float) / window\n",
        "    return np.convolve(x, kernel, mode=\"same\")\n",
        "\n",
        "\n",
        "def _safe_makedirs(path: str) -> None:\n",
        "    if path:\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "# def plot_training_dashboard(\n",
        "#     *,\n",
        "#     train_steps: List[int],\n",
        "#     train_losses: List[float],\n",
        "#     train_tokens_per_s: List[float],\n",
        "#     val_steps: List[int],\n",
        "#     val_losses: List[float],\n",
        "#     lr_steps: List[int],\n",
        "#     lrs_by_group: Dict[str, List[float]],\n",
        "#     epoch_end_steps: List[int],\n",
        "#     epoch_summaries: List[Dict[str, float]],\n",
        "#     title: str,\n",
        "#     smooth_window: int = 50,\n",
        "#     out_path: str = \"plots/training_dashboard.png\",\n",
        "#     show: bool = False,\n",
        "# ) -> None:\n",
        "#     _safe_makedirs(os.path.dirname(out_path))\n",
        "\n",
        "#     steps = np.asarray(train_steps, dtype=int)\n",
        "#     tr_loss = np.asarray(train_losses, dtype=float)\n",
        "#     tr_tps = np.asarray(train_tokens_per_s, dtype=float)\n",
        "\n",
        "#     tr_loss_s = _moving_average(tr_loss, smooth_window)\n",
        "#     tr_tps_s = _moving_average(tr_tps, smooth_window)\n",
        "\n",
        "#     fig = plt.figure(figsize=(16, 10))\n",
        "#     gs = fig.add_gridspec(nrows=3, ncols=2, height_ratios=[2.2, 1.2, 1.2])\n",
        "\n",
        "#     # 1) Loss panel (train + val)\n",
        "#     ax_loss = fig.add_subplot(gs[0, :])\n",
        "#     ax_loss.plot(steps, tr_loss, alpha=0.25, label=\"Train loss (raw)\")\n",
        "#     ax_loss.plot(steps, tr_loss_s, linewidth=2.0, label=f\"Train loss (smoothed, w={smooth_window})\")\n",
        "\n",
        "#     if len(val_steps) > 0:\n",
        "#         v_steps = np.asarray(val_steps, dtype=int)\n",
        "#         v_loss = np.asarray(val_losses, dtype=float)\n",
        "#         ax_loss.plot(v_steps, v_loss, marker=\"o\", linewidth=1.6, label=\"Val loss\")\n",
        "\n",
        "#         best_i = int(np.argmin(v_loss))\n",
        "#         ax_loss.scatter([v_steps[best_i]], [v_loss[best_i]], s=90, zorder=6, label=f\"Best val: {v_loss[best_i]:.4f}\")\n",
        "\n",
        "#     for s in epoch_end_steps:\n",
        "#         ax_loss.axvline(s, linestyle=\"--\", alpha=0.15)\n",
        "\n",
        "#     ax_loss.set_title(title)\n",
        "#     ax_loss.set_xlabel(\"Global step\")\n",
        "#     ax_loss.set_ylabel(\"Loss\")\n",
        "#     ax_loss.grid(True, alpha=0.25)\n",
        "#     ax_loss.legend(loc=\"best\")\n",
        "\n",
        "#     # 2) Throughput panel\n",
        "#     ax_tps = fig.add_subplot(gs[1, 0])\n",
        "#     ax_tps.plot(steps, tr_tps, alpha=0.25, label=\"Tokens/s (raw)\")\n",
        "#     ax_tps.plot(steps, tr_tps_s, linewidth=2.0, label=f\"Tokens/s (smoothed, w={smooth_window})\")\n",
        "#     for s in epoch_end_steps:\n",
        "#         ax_tps.axvline(s, linestyle=\"--\", alpha=0.15)\n",
        "#     ax_tps.set_xlabel(\"Global step\")\n",
        "#     ax_tps.set_ylabel(\"Tokens/s\")\n",
        "#     ax_tps.grid(True, alpha=0.25)\n",
        "#     ax_tps.legend(loc=\"best\")\n",
        "\n",
        "#     # 3) LR panel (multiple param groups)\n",
        "#     ax_lr = fig.add_subplot(gs[1, 1])\n",
        "#     lr_x = np.asarray(lr_steps, dtype=int) if len(lr_steps) > 0 else np.asarray(train_steps, dtype=int)\n",
        "#     for name, series in lrs_by_group.items():\n",
        "#         if len(series) == 0:\n",
        "#             continue\n",
        "#         ax_lr.plot(lr_x[: len(series)], np.asarray(series, dtype=float), linewidth=2.0, label=f\"LR: {name}\")\n",
        "#     for s in epoch_end_steps:\n",
        "#         ax_lr.axvline(s, linestyle=\"--\", alpha=0.15)\n",
        "#     ax_lr.set_xlabel(\"Global step\")\n",
        "#     ax_lr.set_ylabel(\"Learning rate\")\n",
        "#     ax_lr.grid(True, alpha=0.25)\n",
        "#     ax_lr.legend(loc=\"best\")\n",
        "\n",
        "#     # 4) Epoch summary table panel\n",
        "#     ax_tbl = fig.add_subplot(gs[2, :])\n",
        "#     ax_tbl.axis(\"off\")\n",
        "\n",
        "#     if len(epoch_summaries) > 0:\n",
        "#         # show last up to 8 epochs\n",
        "#         tail = epoch_summaries[-8:]\n",
        "#         cols = [\"epoch\", \"train_loss\", \"val_loss\", \"epoch_time_s\", \"tokens_per_s\"]\n",
        "#         cell_text = []\n",
        "#         for row in tail:\n",
        "#             cell_text.append([\n",
        "#                 int(row.get(\"epoch\", -1)),\n",
        "#                 f'{row.get(\"train_loss\", float(\"nan\")):.4f}',\n",
        "#                 f'{row.get(\"val_loss\", float(\"nan\")):.4f}',\n",
        "#                 f'{row.get(\"epoch_time_s\", float(\"nan\")):.2f}',\n",
        "#                 f'{row.get(\"tokens_per_s\", float(\"nan\")):.2f}',\n",
        "#             ])\n",
        "\n",
        "#         table = ax_tbl.table(\n",
        "#             cellText=cell_text,\n",
        "#             colLabels=cols,\n",
        "#             cellLoc=\"center\",\n",
        "#             colLoc=\"center\",\n",
        "#             loc=\"center\",\n",
        "#         )\n",
        "#         table.auto_set_font_size(False)\n",
        "#         table.set_fontsize(10)\n",
        "#         table.scale(1, 1.4)\n",
        "#         ax_tbl.set_title(\"Recent epoch summary\")\n",
        "\n",
        "#     fig.tight_layout()\n",
        "#     fig.savefig(out_path, dpi=220, bbox_inches=\"tight\")\n",
        "#     if show:\n",
        "#         plt.show()\n",
        "#     plt.close(fig)\n",
        "\n",
        "def ema_smooth(x, alpha=0.06):\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    if x.size == 0:\n",
        "        return x\n",
        "    y = np.empty_like(x)\n",
        "    y[0] = x[0]\n",
        "    for i in range(1, len(x)):\n",
        "        y[i] = alpha * x[i] + (1 - alpha) * y[i - 1]\n",
        "    return y\n",
        "\n",
        "\n",
        "def ensure_dir(path: str):\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "\n",
        "def plot_training_dashboard_presentation(\n",
        "    *,\n",
        "    train_steps,\n",
        "    train_losses,\n",
        "    train_tokens_per_s,\n",
        "    val_steps,\n",
        "    val_losses,\n",
        "    lr_steps,\n",
        "    lrs_by_group,\n",
        "    epoch_end_steps,\n",
        "    title=\"nanoVLM Training Dashboard\",\n",
        "    subtitle=None,\n",
        "    out_dir=\"/content/plots\",\n",
        "    filename_prefix=\"training_dashboard\",\n",
        "    smooth_alpha=0.06,\n",
        "    loss_log_scale=False,\n",
        "    show=False,\n",
        "):\n",
        "    ensure_dir(out_dir)\n",
        "\n",
        "    steps = np.asarray(train_steps, dtype=int)\n",
        "    tr_loss = np.asarray(train_losses, dtype=float)\n",
        "    tr_tps = np.asarray(train_tokens_per_s, dtype=float)\n",
        "\n",
        "    tr_loss_s = ema_smooth(tr_loss, alpha=smooth_alpha)\n",
        "    tr_tps_s = ema_smooth(tr_tps, alpha=smooth_alpha)\n",
        "\n",
        "    has_val = len(val_steps) > 0 and len(val_losses) > 0\n",
        "    if has_val:\n",
        "        v_steps = np.asarray(val_steps, dtype=int)\n",
        "        v_loss = np.asarray(val_losses, dtype=float)\n",
        "        best_i = int(np.argmin(v_loss))\n",
        "        best_step = int(v_steps[best_i])\n",
        "        best_val = float(v_loss[best_i])\n",
        "        last_val = float(v_loss[-1])\n",
        "    else:\n",
        "        best_step, best_val, last_val = None, None, None\n",
        "\n",
        "    # Styling for presentation\n",
        "    plt.rcParams.update({\n",
        "        \"figure.dpi\": 120,\n",
        "        \"savefig.dpi\": 250,\n",
        "        \"font.size\": 11,\n",
        "        \"axes.titlesize\": 16,\n",
        "        \"axes.labelsize\": 12,\n",
        "        \"legend.fontsize\": 10,\n",
        "        \"xtick.labelsize\": 10,\n",
        "        \"ytick.labelsize\": 10,\n",
        "    })\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 10))\n",
        "    gs = fig.add_gridspec(nrows=3, ncols=2, height_ratios=[2.25, 1.1, 1.15], hspace=0.35, wspace=0.25)\n",
        "\n",
        "    # Top: Loss plot\n",
        "    ax_loss = fig.add_subplot(gs[0, :])\n",
        "    ax_loss.plot(steps, tr_loss, alpha=0.22, linewidth=1.0, label=\"Train loss (raw)\")\n",
        "    ax_loss.plot(steps, tr_loss_s, linewidth=2.3, label=f\"Train loss (EMA, alpha={smooth_alpha})\")\n",
        "\n",
        "    if has_val:\n",
        "        ax_loss.plot(v_steps, v_loss, marker=\"o\", markersize=4.5, linewidth=1.8, label=\"Val loss\")\n",
        "        ax_loss.scatter([best_step], [best_val], s=90, zorder=5, label=f\"Best val = {best_val:.4f}\")\n",
        "\n",
        "        ax_loss.annotate(\n",
        "            f\"Best val\\n{best_val:.4f} @ step {best_step}\",\n",
        "            xy=(best_step, best_val),\n",
        "            xytext=(10, 15),\n",
        "            textcoords=\"offset points\",\n",
        "            bbox=dict(boxstyle=\"round,pad=0.3\", alpha=0.15),\n",
        "            arrowprops=dict(arrowstyle=\"->\", alpha=0.5),\n",
        "        )\n",
        "\n",
        "    for s in epoch_end_steps:\n",
        "        ax_loss.axvline(s, linestyle=\"--\", alpha=0.10)\n",
        "\n",
        "    ax_loss.set_title(title)\n",
        "    if subtitle:\n",
        "        ax_loss.text(0.0, 1.02, subtitle, transform=ax_loss.transAxes, fontsize=11, alpha=0.8)\n",
        "\n",
        "    ax_loss.set_xlabel(\"Global step\")\n",
        "    ax_loss.set_ylabel(\"Loss\")\n",
        "    if loss_log_scale:\n",
        "        ax_loss.set_yscale(\"log\")\n",
        "        ax_loss.set_ylabel(\"Loss (log scale)\")\n",
        "\n",
        "    ax_loss.grid(True, alpha=0.18)\n",
        "    ax_loss.legend(loc=\"best\")\n",
        "\n",
        "    # Middle left: Tokens/s\n",
        "    ax_tps = fig.add_subplot(gs[1, 0])\n",
        "    ax_tps.plot(steps, tr_tps, alpha=0.22, linewidth=1.0, label=\"Tokens/s (raw)\")\n",
        "    ax_tps.plot(steps, tr_tps_s, linewidth=2.3, label=f\"Tokens/s (EMA, alpha={smooth_alpha})\")\n",
        "    for s in epoch_end_steps:\n",
        "        ax_tps.axvline(s, linestyle=\"--\", alpha=0.10)\n",
        "    ax_tps.set_xlabel(\"Global step\")\n",
        "    ax_tps.set_ylabel(\"Tokens/s\")\n",
        "    ax_tps.grid(True, alpha=0.18)\n",
        "    ax_tps.legend(loc=\"best\")\n",
        "\n",
        "    # Middle right: Learning rates\n",
        "    ax_lr = fig.add_subplot(gs[1, 1])\n",
        "    if lr_steps and lrs_by_group and any(len(v) > 0 for v in lrs_by_group.values()):\n",
        "        lr_x = np.asarray(lr_steps, dtype=int)\n",
        "        for name, series in lrs_by_group.items():\n",
        "            if len(series) == 0:\n",
        "                continue\n",
        "            y = np.asarray(series, dtype=float)\n",
        "            ax_lr.plot(lr_x[: len(y)], y, linewidth=2.3, label=f\"{name}\")\n",
        "        for s in epoch_end_steps:\n",
        "            ax_lr.axvline(s, linestyle=\"--\", alpha=0.10)\n",
        "        ax_lr.set_xlabel(\"Global step\")\n",
        "        ax_lr.set_ylabel(\"Learning rate\")\n",
        "        ax_lr.grid(True, alpha=0.18)\n",
        "        ax_lr.legend(loc=\"best\", title=\"Param groups\")\n",
        "    else:\n",
        "        ax_lr.text(0.5, 0.5, \"LR curves not logged\", ha=\"center\", va=\"center\", alpha=0.7)\n",
        "        ax_lr.axis(\"off\")\n",
        "\n",
        "    # Bottom: summary box + small epoch table\n",
        "    ax_bottom = fig.add_subplot(gs[2, :])\n",
        "    ax_bottom.axis(\"off\")\n",
        "\n",
        "    last_train = float(tr_loss_s[-1]) if tr_loss_s.size else float(\"nan\")\n",
        "    peak_tps = float(np.nanmax(tr_tps)) if tr_tps.size else float(\"nan\")\n",
        "    last_tps = float(tr_tps_s[-1]) if tr_tps_s.size else float(\"nan\")\n",
        "\n",
        "    summary_lines = [\n",
        "        f\"Last train loss (EMA): {last_train:.4f}\",\n",
        "        f\"Last tokens/s (EMA): {last_tps:.1f}\",\n",
        "        f\"Peak tokens/s: {peak_tps:.1f}\",\n",
        "    ]\n",
        "    if has_val:\n",
        "        summary_lines.insert(1, f\"Best val loss: {best_val:.4f} (step {best_step})\")\n",
        "        summary_lines.insert(2, f\"Last val loss: {last_val:.4f}\")\n",
        "\n",
        "    summary_text = \"\\n\".join(summary_lines)\n",
        "    ax_bottom.text(\n",
        "        0.01, 0.88, \"Run Summary\",\n",
        "        fontsize=13, weight=\"bold\", alpha=0.9, transform=ax_bottom.transAxes\n",
        "    )\n",
        "    ax_bottom.text(\n",
        "        0.01, 0.16, summary_text,\n",
        "        fontsize=11, transform=ax_bottom.transAxes,\n",
        "        bbox=dict(boxstyle=\"round,pad=0.5\", alpha=0.10)\n",
        "    )\n",
        "\n",
        "    # Add a small \"last 6 evals\" table if val exists\n",
        "    if has_val:\n",
        "        k = min(6, len(v_steps))\n",
        "        tail_steps = v_steps[-k:]\n",
        "        tail_vals = v_loss[-k:]\n",
        "        cell_text = [[int(s), f\"{float(l):.4f}\"] for s, l in zip(tail_steps, tail_vals)]\n",
        "        table = ax_bottom.table(\n",
        "            cellText=cell_text,\n",
        "            colLabels=[\"val_step\", \"val_loss\"],\n",
        "            cellLoc=\"center\",\n",
        "            colLoc=\"center\",\n",
        "            bbox=[0.62, 0.20, 0.37, 0.70],  # [x, y, w, h] in axes coords\n",
        "        )\n",
        "        table.auto_set_font_size(False)\n",
        "        table.set_fontsize(10)\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "    png_path = os.path.join(out_dir, f\"{filename_prefix}.png\")\n",
        "    pdf_path = os.path.join(out_dir, f\"{filename_prefix}.pdf\")\n",
        "    fig.savefig(png_path, bbox_inches=\"tight\")\n",
        "    fig.savefig(pdf_path, bbox_inches=\"tight\")\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    plt.close(fig)\n",
        "\n",
        "    return png_path, pdf_path\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Training loop (edited)\n",
        "# ----------------------------\n",
        "def train(train_cfg, vlm_cfg):\n",
        "    train_loader, val_loader = get_dataloaders(train_cfg, vlm_cfg)\n",
        "\n",
        "    # Initialize model\n",
        "    if train_cfg.resume_from_vlm_checkpoint:\n",
        "        print(f\"Resuming from VLM checkpoint: {vlm_cfg.vlm_checkpoint_path}\")\n",
        "        model = VisionLanguageModel.from_pretrained(vlm_cfg.vlm_checkpoint_path)\n",
        "    else:\n",
        "        model = VisionLanguageModel(vlm_cfg, load_backbone=vlm_cfg.vlm_load_backbone_weights)\n",
        "\n",
        "    print(f\"nanoVLM initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
        "    print(f\"Training summary: {len(train_loader.dataset)} samples, {len(train_loader)} batches/epoch, batch size {train_cfg.batch_size}\")\n",
        "\n",
        "    # Param groups\n",
        "    param_groups = []\n",
        "    group_names = []\n",
        "\n",
        "    if train_cfg.lr_mp > 0:\n",
        "        param_groups.append({\"params\": list(model.MP.parameters()), \"lr\": train_cfg.lr_mp})\n",
        "        group_names.append(\"MP\")\n",
        "    else:\n",
        "        for p in list(model.MP.parameters()):\n",
        "            p.requires_grad = False\n",
        "\n",
        "    if train_cfg.lr_vision_backbone > 0:\n",
        "        param_groups.append({\"params\": list(model.vision_encoder.parameters()), \"lr\": train_cfg.lr_vision_backbone})\n",
        "        group_names.append(\"vision_backbone\")\n",
        "    else:\n",
        "        for p in list(model.vision_encoder.parameters()):\n",
        "            p.requires_grad = False\n",
        "\n",
        "    if train_cfg.lr_language_backbone > 0:\n",
        "        param_groups.append({\"params\": list(model.decoder.parameters()), \"lr\": train_cfg.lr_language_backbone})\n",
        "        group_names.append(\"language_backbone\")\n",
        "    else:\n",
        "        for p in list(model.decoder.parameters()):\n",
        "            p.requires_grad = False\n",
        "\n",
        "    optimizer = optim.AdamW(param_groups)\n",
        "    all_params = [p for group in optimizer.param_groups for p in group[\"params\"]]\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    if train_cfg.compile:\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    # Logs for dashboard\n",
        "    train_steps: List[int] = []\n",
        "    train_losses: List[float] = []\n",
        "    train_tokens_per_s: List[float] = []\n",
        "\n",
        "    val_steps: List[int] = []\n",
        "    val_losses: List[float] = []\n",
        "\n",
        "    lr_steps: List[int] = []\n",
        "    lrs_by_group: Dict[str, List[float]] = {name: [] for name in group_names}\n",
        "\n",
        "    epoch_end_steps: List[int] = []\n",
        "    epoch_summaries: List[Dict[str, float]] = []\n",
        "\n",
        "    epoch_times: List[float] = []\n",
        "\n",
        "    global_step = 0\n",
        "    epoch = 0\n",
        "\n",
        "    # Where to save plots\n",
        "    plots_dir = getattr(train_cfg, \"plots_dir\", \"plots\")\n",
        "    _safe_makedirs(plots_dir)\n",
        "\n",
        "    last_avg_val_loss = float(\"nan\")\n",
        "\n",
        "    while global_step < train_cfg.max_training_steps:\n",
        "        epoch_start_time = time.time()\n",
        "        epoch += 1\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        total_tokens_processed = 0.0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(\"Starting training loop\")\n",
        "        for i, batch in enumerate(synchronized_dataloader_step(train_loader, False)):\n",
        "            if global_step >= train_cfg.max_training_steps:\n",
        "                break\n",
        "\n",
        "            batch_start_time = time.time()\n",
        "            is_update_step = (i + 1) % train_cfg.gradient_accumulation_steps == 0 or (i + 1) == len(train_loader)\n",
        "\n",
        "            images = batch[\"images\"]\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "\n",
        "            with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                _, loss = model(input_ids, images, attention_mask=attention_mask, targets=labels)\n",
        "\n",
        "            if train_cfg.gradient_accumulation_steps > 1:\n",
        "                loss = loss / train_cfg.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            if is_update_step:\n",
        "                if train_cfg.max_grad_norm is not None:\n",
        "                    _ = torch.nn.utils.clip_grad_norm_(all_params, max_norm=train_cfg.max_grad_norm)\n",
        "\n",
        "                # Update LR per group\n",
        "                param_group_idx = 0\n",
        "                if train_cfg.lr_mp > 0:\n",
        "                    optimizer.param_groups[param_group_idx][\"lr\"] = get_lr(global_step, train_cfg.lr_mp, train_cfg.max_training_steps)\n",
        "                    param_group_idx += 1\n",
        "                if train_cfg.lr_vision_backbone > 0:\n",
        "                    optimizer.param_groups[param_group_idx][\"lr\"] = get_lr(global_step, train_cfg.lr_vision_backbone, train_cfg.max_training_steps)\n",
        "                    param_group_idx += 1\n",
        "                if train_cfg.lr_language_backbone > 0:\n",
        "                    optimizer.param_groups[param_group_idx][\"lr\"] = get_lr(global_step, train_cfg.lr_language_backbone, train_cfg.max_training_steps)\n",
        "\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Log LRs (only meaningful on update steps)\n",
        "                lr_steps.append(global_step)\n",
        "                for idx, name in enumerate(group_names):\n",
        "                    lrs_by_group[name].append(float(optimizer.param_groups[idx][\"lr\"]))\n",
        "\n",
        "            # Loss logging (use non-scaled loss)\n",
        "            batch_loss = float(loss.item())\n",
        "            if train_cfg.gradient_accumulation_steps > 1:\n",
        "                batch_loss = batch_loss * train_cfg.gradient_accumulation_steps\n",
        "\n",
        "            total_train_loss += batch_loss\n",
        "\n",
        "            num_tokens = float(torch.sum(attention_mask).item())\n",
        "            total_tokens_processed += num_tokens\n",
        "\n",
        "            batch_duration = max(1e-8, time.time() - batch_start_time)\n",
        "            tokens_per_second = num_tokens / batch_duration\n",
        "\n",
        "            train_steps.append(global_step)\n",
        "            train_losses.append(batch_loss)\n",
        "            train_tokens_per_s.append(tokens_per_second)\n",
        "\n",
        "            # Validation every N steps\n",
        "            if global_step % 20 == 0:\n",
        "                model.eval()\n",
        "                torch.cuda.empty_cache()\n",
        "                with torch.no_grad():\n",
        "                    total_val_loss = 0.0\n",
        "                    for vb in synchronized_dataloader_step(val_loader, False):\n",
        "                        v_images = vb[\"images\"]\n",
        "                        v_input_ids = vb[\"input_ids\"].to(device)\n",
        "                        v_labels = vb[\"labels\"].to(device)\n",
        "                        v_attention_mask = vb[\"attention_mask\"].to(device)\n",
        "\n",
        "                        with torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                            _, vloss = model(v_input_ids, v_images, attention_mask=v_attention_mask, targets=v_labels)\n",
        "\n",
        "                        total_val_loss += float(vloss.item())\n",
        "\n",
        "                    avg_val_loss = total_val_loss / max(1, len(val_loader))\n",
        "                    last_avg_val_loss = avg_val_loss\n",
        "                    val_steps.append(global_step)\n",
        "                    val_losses.append(avg_val_loss)\n",
        "\n",
        "                print(f\"\\nStep: {global_step}, Loss: {batch_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Tokens/s: {tokens_per_second:.2f}\")\n",
        "                model.train()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        avg_train_loss = total_train_loss / max(1, len(train_loader))\n",
        "        epoch_duration = time.time() - epoch_start_time\n",
        "        epoch_times.append(epoch_duration)\n",
        "\n",
        "        epoch_tokens_per_second = total_tokens_processed / max(1e-8, epoch_duration)\n",
        "\n",
        "        epoch_end_steps.append(global_step - 1)\n",
        "        epoch_summaries.append({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            \"val_loss\": float(last_avg_val_loss) if not np.isnan(last_avg_val_loss) else float(\"nan\"),\n",
        "            \"epoch_time_s\": epoch_duration,\n",
        "            \"tokens_per_s\": epoch_tokens_per_second,\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch} | Train Loss tah: {avg_train_loss:.4f} | Val Loss: {last_avg_val_loss:.4f} | \"\n",
        "            f\"Time: {epoch_duration:.2f}s | T/s: {epoch_tokens_per_second:.2f}\"\n",
        "        )\n",
        "\n",
        "        # Save a dashboard image each epoch (great for reporting)\n",
        "        # plot_training_dashboard_presentation(\n",
        "        #     train_steps=train_steps,\n",
        "        #     train_losses=train_losses,\n",
        "        #     train_tokens_per_s=train_tokens_per_s,\n",
        "        #     val_steps=val_steps,\n",
        "        #     val_losses=val_losses,\n",
        "        #     lr_steps=lr_steps,\n",
        "        #     lrs_by_group=lrs_by_group,\n",
        "        #     epoch_end_steps=epoch_end_steps,\n",
        "        #     epoch_summaries=epoch_summaries,\n",
        "        #     title=\"nanoVLM Training Dashboard\",\n",
        "        #     smooth_window=50,\n",
        "        #     out_path=os.path.join(plots_dir, f\"dashboard_epoch_{epoch:03d}.png\"),\n",
        "        #     show=False,\n",
        "        # )\n",
        "        png_path, pdf_path = plot_training_dashboard_presentation(\n",
        "        train_steps=train_steps,\n",
        "        train_losses=train_losses,\n",
        "        train_tokens_per_s=train_tokens_per_s,\n",
        "        val_steps=val_steps,\n",
        "        val_losses=val_losses,\n",
        "        lr_steps=lr_steps,\n",
        "        lrs_by_group=lrs_by_group,\n",
        "        epoch_end_steps=epoch_end_steps,\n",
        "        title=\"nanoVLM Training Dashboard\",\n",
        "        subtitle=f\"Batch={train_cfg.batch_size} | GradAcc={train_cfg.gradient_accumulation_steps} | GPU={torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\",\n",
        "        out_dir=plots_dir,\n",
        "        filename_prefix=\"training_dashboard_final\",\n",
        "        smooth_alpha=0.06,\n",
        "        loss_log_scale=False,   # set True if you want log loss\n",
        "        show=True,              # show in notebook\n",
        "        )\n",
        "        print(\"Saved:\", png_path, pdf_path)\n",
        "\n",
        "    # Save and optionally push\n",
        "    model.save_pretrained(save_directory=vlm_cfg.vlm_checkpoint_path)\n",
        "\n",
        "    # If you push to HF, make sure username/token permissions are correct\n",
        "    # model.push_to_hub(hf_model_name)\n",
        "\n",
        "    total_training_time = float(sum(epoch_times))\n",
        "    print(f\"Total training time: {total_training_time:.2f}s\")\n",
        "\n",
        "    # Final combined plot (latest snapshot)\n",
        "    # plot_training_dashboard_presentation(\n",
        "    #     train_steps=train_steps,\n",
        "    #     train_losses=train_losses,\n",
        "    #     train_tokens_per_s=train_tokens_per_s,\n",
        "    #     val_steps=val_steps,\n",
        "    #     val_losses=val_losses,\n",
        "    #     lr_steps=lr_steps,\n",
        "    #     lrs_by_group=lrs_by_group,\n",
        "    #     epoch_end_steps=epoch_end_steps,\n",
        "    #     epoch_summaries=epoch_summaries,\n",
        "    #     title=\"nanoVLM Training Dashboard (Final)\",\n",
        "    #     smooth_window=50,\n",
        "    #     out_path=os.path.join(plots_dir, \"training_dashboard_final.png\"),\n",
        "    #     show=True,\n",
        "    # )\n",
        "    png_path, pdf_path = plot_training_dashboard_presentation(\n",
        "    train_steps=train_steps,\n",
        "    train_losses=train_losses,\n",
        "    train_tokens_per_s=train_tokens_per_s,\n",
        "    val_steps=val_steps,\n",
        "    val_losses=val_losses,\n",
        "    lr_steps=lr_steps,\n",
        "    lrs_by_group=lrs_by_group,\n",
        "    epoch_end_steps=epoch_end_steps,\n",
        "    title=\"nanoVLM Training Dashboard\",\n",
        "    subtitle=f\"Batch={train_cfg.batch_size} | GradAcc={train_cfg.gradient_accumulation_steps} | GPU={torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\",\n",
        "    out_dir=plots_dir,\n",
        "    filename_prefix=\"training_dashboard_final\",\n",
        "    smooth_alpha=0.06,\n",
        "    loss_log_scale=False,   # set True if you want log loss\n",
        "    show=True,              # show in notebook\n",
        "    )\n",
        "    print(\"Saved:\", png_path, pdf_path)\n"
      ],
      "metadata": {
        "id": "k9BhI_4SG7hE"
      },
      "id": "k9BhI_4SG7hE",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d4HmsEPNQZbh",
      "metadata": {
        "id": "d4HmsEPNQZbh"
      },
      "source": [
        "### Prepare the Configs\n",
        "Instead of using the config.py file in the repo (which was created to run on one H100), we will create our config here to play around with the parameters easier and adapt them to colabs capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "h8FlqtizQdO-",
      "metadata": {
        "id": "h8FlqtizQdO-"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class VLMConfig:\n",
        "    vit_hidden_dim: int = 768\n",
        "    vit_inter_dim: int = 4 * vit_hidden_dim\n",
        "    vit_patch_size: int = 16\n",
        "    vit_img_size: int = 512\n",
        "    vit_n_heads: int = 12\n",
        "    vit_dropout: float = 0.0\n",
        "    vit_n_blocks: int = 12\n",
        "    vit_ln_eps: float = 1e-6\n",
        "    vit_cls_flag: bool = False\n",
        "    vit_model_type: str = 'google/siglip2-base-patch16-512'\n",
        "\n",
        "    lm_hidden_dim: int = 960\n",
        "    lm_inter_dim: int = 2560\n",
        "    lm_rms_eps: float = 1e-5\n",
        "    lm_re_base: int = 100000\n",
        "    lm_max_position_embeddings: int = 8192\n",
        "    lm_base_vocab_size: int = 49152\n",
        "    extra_token_amount: int = 66  # Number of extra tokens for the VLM (image start, image end, image token)\n",
        "    lm_vocab_size: int = lm_base_vocab_size + extra_token_amount # Not a great way to do this, but it works for now (vlm_extra_tokens cannot be a dict, since this is mutable, and a Field has no len() function)\n",
        "    lm_n_heads: int = 15\n",
        "    lm_n_kv_heads: int = 5\n",
        "    lm_dropout: float = 0.0\n",
        "    lm_n_blocks: int = 32\n",
        "    lm_attn_scaling: float = 1.0\n",
        "    lm_max_length: int = 256\n",
        "    lm_use_tokens: bool = False # Decide if the LM expects tokens or embeddings as input (if using as a backbone for the VLM, set to False)\n",
        "    lm_tie_weights: bool = True # Decide if you want to tie the LM Head weight to the token embedding weights\n",
        "    lm_model_type: str = 'HuggingFaceTB/SmolLM2-135M'\n",
        "    lm_tokenizer: str = 'HuggingFaceTB/SmolLM2-360M-Instruct'\n",
        "    lm_chat_template: str = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
        "\n",
        "    mp_pixel_shuffle_factor: int = 4\n",
        "    mp_image_token_length: int = 64\n",
        "\n",
        "    max_img_size: int = 512\n",
        "    resize_to_max_side_len: bool = False\n",
        "\n",
        "    vlm_extra_tokens: dict[str, str] = field(default_factory=lambda: {\"image_token\": \"<|image|>\", \"global_image_token\": \"<|global_image|>\",\n",
        "      \"r1c1\": \"<row_1_col_1>\", \"r1c2\": \"<row_1_col_2>\", \"r1c3\": \"<row_1_col_3>\", \"r1c4\": \"<row_1_col_4>\", \"r1c5\": \"<row_1_col_5>\", \"r1c6\": \"<row_1_col_6>\", \"r1c7\": \"<row_1_col_7>\", \"r1c8\": \"<row_1_col_8>\",\n",
        "      \"r2c1\": \"<row_2_col_1>\", \"r2c2\": \"<row_2_col_2>\", \"r2c3\": \"<row_2_col_3>\", \"r2c4\": \"<row_2_col_4>\", \"r2c5\": \"<row_2_col_5>\", \"r2c6\": \"<row_2_col_6>\", \"r2c7\": \"<row_2_col_7>\", \"r2c8\": \"<row_2_col_8>\",\n",
        "      \"r3c1\": \"<row_3_col_1>\", \"r3c2\": \"<row_3_col_2>\", \"r3c3\": \"<row_3_col_3>\", \"r3c4\": \"<row_3_col_4>\", \"r3c5\": \"<row_3_col_5>\", \"r3c6\": \"<row_3_col_6>\", \"r3c7\": \"<row_3_col_7>\", \"r3c8\": \"<row_3_col_8>\",\n",
        "      \"r4c1\": \"<row_4_col_1>\", \"r4c2\": \"<row_4_col_2>\", \"r4c3\": \"<row_4_col_3>\", \"r4c4\": \"<row_4_col_4>\", \"r4c5\": \"<row_4_col_5>\", \"r4c6\": \"<row_4_col_6>\", \"r4c7\": \"<row_4_col_7>\", \"r4c8\": \"<row_4_col_8>\",\n",
        "      \"r5c1\": \"<row_5_col_1>\", \"r5c2\": \"<row_5_col_2>\", \"r5c3\": \"<row_5_col_3>\", \"r5c4\": \"<row_5_col_4>\", \"r5c5\": \"<row_5_col_5>\", \"r5c6\": \"<row_5_col_6>\", \"r5c7\": \"<row_5_col_7>\", \"r5c8\": \"<row_5_col_8>\",\n",
        "      \"r6c1\": \"<row_6_col_1>\", \"r6c2\": \"<row_6_col_2>\", \"r6c3\": \"<row_6_col_3>\", \"r6c4\": \"<row_6_col_4>\", \"r6c5\": \"<row_6_col_5>\", \"r6c6\": \"<row_6_col_6>\", \"r6c7\": \"<row_6_col_7>\", \"r6c8\": \"<row_6_col_8>\",\n",
        "      \"r7c1\": \"<row_7_col_1>\", \"r7c2\": \"<row_7_col_2>\", \"r7c3\": \"<row_7_col_3>\", \"r7c4\": \"<row_7_col_4>\", \"r7c5\": \"<row_7_col_5>\", \"r7c6\": \"<row_7_col_6>\", \"r7c7\": \"<row_7_col_7>\", \"r7c8\": \"<row_7_col_8>\",\n",
        "      \"r8c1\": \"<row_8_col_1>\", \"r8c2\": \"<row_8_col_2>\", \"r8c3\": \"<row_8_col_3>\", \"r8c4\": \"<row_8_col_4>\", \"r8c5\": \"<row_8_col_5>\", \"r8c6\": \"<row_8_col_6>\", \"r8c7\": \"<row_8_col_7>\", \"r8c8\": \"<row_8_col_8>\"})\n",
        "    vlm_load_backbone_weights: bool = True\n",
        "    vlm_checkpoint_path: str = 'checkpoints'\n",
        "    hf_repo_name: str = 'nanoVLM'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    lr_mp: float = 0.005\n",
        "    lr_vision_backbone: float = 0.0005\n",
        "    lr_language_backbone: float = 0.0005\n",
        "    data_cutoff_idx: int = 256 # Let's only use a small subset at first\n",
        "    val_ratio: float = 0.2\n",
        "    batch_size: int = 4\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    max_grad_norm: float = 1.0\n",
        "    max_training_steps: int = 80\n",
        "    max_images_per_example: int = 2\n",
        "    max_images_per_knapsack: int = 8\n",
        "    max_sample_length: int = 256\n",
        "    compile: bool = False\n",
        "    resume_from_vlm_checkpoint: bool = False # Indicate if the training should be resumed from a checkpoint of the whole VLM or you want to start from scratch\n",
        "    train_dataset_path: str = 'HuggingFaceM4/the_cauldron'\n",
        "    train_dataset_name: tuple[str, ...] = (\"tqa\", ) #All options; (\"ai2d\", \"aokvqa\", \"chart2text\", \"chartqa\", \"clevr\", \"cocoqa\", \"datikz\", \"diagram_image_to_text\", \"docvqa\", \"dvqa\", \"figureqa\", \"finqa\", \"geomverse\", \"hateful_memes\", \"hitab\", \"iam\", \"iconqa\", \"infographic_vqa\", \"intergps\", \"localized_narratives\", \"mapqa\", \"multihiertt\", \"ocrvqa\", \"plotqa\", \"raven\", \"rendered_text\", \"robut_sqa\", \"robut_wikisql\", \"robut_wtq\", \"scienceqa\", \"screen2words\", \"st_vqa\", \"tabmwp\", \"tallyqa\", \"tat_qa\", \"textcaps\", \"textvqa\", \"tqa\", \"vistext\", \"visual7w\", \"visualmrc\", \"vqarad\", \"vqav2\", \"vsr\", \"websight\") # \"clevr_math\", \"okvqa\", \"spot_the_diff\", \"nlvr2\", \"mimic_cgd\",\n",
        "\n",
        "@dataclass\n",
        "class VLMConfig1:\n",
        "    vit_hidden_dim: int = 768\n",
        "    vit_inter_dim: int = 4 * vit_hidden_dim\n",
        "    vit_patch_size: int = 16\n",
        "    vit_img_size: int = 512\n",
        "    vit_n_heads: int = 12\n",
        "    vit_dropout: float = 0.0\n",
        "    vit_n_blocks: int = 12\n",
        "    vit_ln_eps: float = 1e-6\n",
        "    vit_cls_flag: bool = False\n",
        "    vit_model_type: str = 'google/siglip2-base-patch16-512'\n",
        "\n",
        "    lm_hidden_dim: int = 960\n",
        "    lm_inter_dim: int = 2560\n",
        "    lm_rms_eps: float = 1e-5\n",
        "    lm_re_base: int = 100000\n",
        "    lm_max_position_embeddings: int = 8192\n",
        "    lm_base_vocab_size: int = 49152\n",
        "    extra_token_amount: int = 66  # Number of extra tokens for the VLM (image start, image end, image token)\n",
        "    lm_vocab_size: int = lm_base_vocab_size + extra_token_amount # Not a great way to do this, but it works for now (vlm_extra_tokens cannot be a dict, since this is mutable, and a Field has no len() function)\n",
        "    lm_n_heads: int = 15\n",
        "    lm_n_kv_heads: int = 5\n",
        "    lm_dropout: float = 0.0\n",
        "    lm_n_blocks: int = 32\n",
        "    lm_attn_scaling: float = 1.0\n",
        "    lm_max_length: int = 4096\n",
        "    lm_use_tokens: bool = False # Decide if the LM expects tokens or embeddings as input (if using as a backbone for the VLM, set to False)\n",
        "    lm_tie_weights: bool = True # Decide if you want to tie the LM Head weight to the token embedding weights\n",
        "    lm_model_type: str = 'HuggingFaceTB/SmolLM2-360M-Instruct' #'HuggingFaceTB/SmolLM2-135M' #\n",
        "    lm_tokenizer: str = 'HuggingFaceTB/SmolLM2-360M-Instruct'\n",
        "    lm_chat_template: str = \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
        "\n",
        "    mp_pixel_shuffle_factor: int = 4\n",
        "    mp_image_token_length: int = 64\n",
        "\n",
        "    max_img_size: int = 2048\n",
        "    resize_to_max_side_len: bool = True\n",
        "\n",
        "    vlm_extra_tokens: dict[str, str] = field(default_factory=lambda: {\"image_token\": \"<|image|>\", \"global_image_token\": \"<|global_image|>\",\n",
        "      \"r1c1\": \"<row_1_col_1>\", \"r1c2\": \"<row_1_col_2>\", \"r1c3\": \"<row_1_col_3>\", \"r1c4\": \"<row_1_col_4>\", \"r1c5\": \"<row_1_col_5>\", \"r1c6\": \"<row_1_col_6>\", \"r1c7\": \"<row_1_col_7>\", \"r1c8\": \"<row_1_col_8>\",\n",
        "      \"r2c1\": \"<row_2_col_1>\", \"r2c2\": \"<row_2_col_2>\", \"r2c3\": \"<row_2_col_3>\", \"r2c4\": \"<row_2_col_4>\", \"r2c5\": \"<row_2_col_5>\", \"r2c6\": \"<row_2_col_6>\", \"r2c7\": \"<row_2_col_7>\", \"r2c8\": \"<row_2_col_8>\",\n",
        "      \"r3c1\": \"<row_3_col_1>\", \"r3c2\": \"<row_3_col_2>\", \"r3c3\": \"<row_3_col_3>\", \"r3c4\": \"<row_3_col_4>\", \"r3c5\": \"<row_3_col_5>\", \"r3c6\": \"<row_3_col_6>\", \"r3c7\": \"<row_3_col_7>\", \"r3c8\": \"<row_3_col_8>\",\n",
        "      \"r4c1\": \"<row_4_col_1>\", \"r4c2\": \"<row_4_col_2>\", \"r4c3\": \"<row_4_col_3>\", \"r4c4\": \"<row_4_col_4>\", \"r4c5\": \"<row_4_col_5>\", \"r4c6\": \"<row_4_col_6>\", \"r4c7\": \"<row_4_col_7>\", \"r4c8\": \"<row_4_col_8>\",\n",
        "      \"r5c1\": \"<row_5_col_1>\", \"r5c2\": \"<row_5_col_2>\", \"r5c3\": \"<row_5_col_3>\", \"r5c4\": \"<row_5_col_4>\", \"r5c5\": \"<row_5_col_5>\", \"r5c6\": \"<row_5_col_6>\", \"r5c7\": \"<row_5_col_7>\", \"r5c8\": \"<row_5_col_8>\",\n",
        "      \"r6c1\": \"<row_6_col_1>\", \"r6c2\": \"<row_6_col_2>\", \"r6c3\": \"<row_6_col_3>\", \"r6c4\": \"<row_6_col_4>\", \"r6c5\": \"<row_6_col_5>\", \"r6c6\": \"<row_6_col_6>\", \"r6c7\": \"<row_6_col_7>\", \"r6c8\": \"<row_6_col_8>\",\n",
        "      \"r7c1\": \"<row_7_col_1>\", \"r7c2\": \"<row_7_col_2>\", \"r7c3\": \"<row_7_col_3>\", \"r7c4\": \"<row_7_col_4>\", \"r7c5\": \"<row_7_col_5>\", \"r7c6\": \"<row_7_col_6>\", \"r7c7\": \"<row_7_col_7>\", \"r7c8\": \"<row_7_col_8>\",\n",
        "      \"r8c1\": \"<row_8_col_1>\", \"r8c2\": \"<row_8_col_2>\", \"r8c3\": \"<row_8_col_3>\", \"r8c4\": \"<row_8_col_4>\", \"r8c5\": \"<row_8_col_5>\", \"r8c6\": \"<row_8_col_6>\", \"r8c7\": \"<row_8_col_7>\", \"r8c8\": \"<row_8_col_8>\"})\n",
        "    vlm_load_backbone_weights: bool = True\n",
        "    vlm_checkpoint_path: str = 'checkpoints'\n",
        "    hf_repo_name: str = 'nanoVLM'\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TrainConfig1:\n",
        "    lr_mp: float = 0.005\n",
        "    lr_vision_backbone: float = 0.0005\n",
        "    lr_language_backbone: float = 0.0005\n",
        "    data_cutoff_idx: int = 512 # Let's only use a small subset at first\n",
        "    val_ratio: float = 0.2\n",
        "    batch_size: int = 4\n",
        "    gradient_accumulation_steps: int = 8\n",
        "    max_grad_norm: float = 1.0\n",
        "    max_training_steps: int = 80\n",
        "    max_images_per_example: int = 4\n",
        "    max_images_per_knapsack: int = 18\n",
        "    max_sample_length: int = 512\n",
        "    compile: bool = False\n",
        "    resume_from_vlm_checkpoint: bool = False # Indicate if the training should be resumed from a checkpoint of the whole VLM or you want to start from scratch\n",
        "    train_dataset_path: str = 'HuggingFaceM4/the_cauldron'\n",
        "    train_dataset_name: tuple[str, ...] = (\"tqa\", ) #All options; (\"ai2d\", \"aokvqa\", \"chart2text\", \"chartqa\", \"clevr\", \"cocoqa\", \"datikz\", \"diagram_image_to_text\", \"docvqa\", \"dvqa\", \"figureqa\", \"finqa\", \"geomverse\", \"hateful_memes\", \"hitab\", \"iam\", \"iconqa\", \"infographic_vqa\", \"intergps\", \"localized_narratives\", \"mapqa\", \"multihiertt\", \"ocrvqa\", \"plotqa\", \"raven\", \"rendered_text\", \"robut_sqa\", \"robut_wikisql\", \"robut_wtq\", \"scienceqa\", \"screen2words\", \"st_vqa\", \"tabmwp\", \"tallyqa\", \"tat_qa\", \"textcaps\", \"textvqa\", \"tqa\", \"vistext\", \"visual7w\", \"visualmrc\", \"vqarad\", \"vqav2\", \"vsr\", \"websight\") # \"clevr_math\", \"okvqa\", \"spot_the_diff\", \"nlvr2\", \"mimic_cgd\",\n",
        "\n",
        "    # lr_mp: float = 0.00512\n",
        "    # lr_vision_backbone: float = 5e-5 #0.0005 #\n",
        "    # lr_language_backbone: float = 5e-5 #0\n",
        "    # # val_size: int = 50000\n",
        "    # val_ratio: float = 0.2\n",
        "    # batch_size: int = 4\n",
        "    # gradient_accumulation_steps: int = 8\n",
        "    # max_grad_norm: float = 1.0\n",
        "    # eval_in_epochs: bool = True\n",
        "    # eval_interval: int = 500\n",
        "    # stats_log_interval: int = 100\n",
        "    # max_training_steps: int = 4000\n",
        "    # max_images_per_example: int = 4\n",
        "    # max_images_per_knapsack: int = 18\n",
        "    # max_sample_length: int = 4096\n",
        "    # compile: bool = False\n",
        "    # resume_from_vlm_checkpoint: bool = False # Indicate if the training should be resumed from a checkpoint of the whole VLM or you want to start from scratch\n",
        "    # # train_dataset_path: str = 'HuggingFaceM4/FineVision_concat_shuffled_2'\n",
        "    # # train_dataset_name: tuple[str, ...] = (\"default\", ) #('allava_laion', 'allava_vflan', 'cambrian(filtered)_processed', 'LLaVA_Instruct_150K', 'mmevol', 'sharegpt4o', 'sharegpt4v(coco)', 'sharegpt4v(knowledge)', 'sharegpt4v(llava)', 'sharegpt4v(sam)') # 'vision_flan(filtered)', 'lvis_instruct4v',\n",
        "    # train_dataset_path: str = 'HuggingFaceM4/the_cauldron'\n",
        "    # train_dataset_name: tuple[str, ...] = (\"tqa\", )\n",
        "    # stream_dataset: bool = True\n",
        "    # relevance_min_rating: int = 1\n",
        "    # image_correspondence_min_rating: int = 1\n",
        "    # visual_dependency_min_rating: int = 1\n",
        "    # formatting_min_rating: int = 1\n",
        "    # wandb_entity: str = \"HuggingFace\" # Indicate the entity to log to in wandb\n",
        "    # log_wandb: bool = True\n",
        "    # use_lmms_eval: bool = True # Use lmms-eval for evaluation\n",
        "    # lmms_eval_tasks: str = 'mmstar,mmmu_val,ocrbench,textvqa_val,docvqa_val,scienceqa,mme,infovqa_val,chartqa' # Pass additional task as one string, seperated by commas without spaces (e.g. 'mmstar,mmmu,ocrbench')\n",
        "    # lmms_eval_limit: float = None\n",
        "    # lmms_eval_batch_size: int = 64\n",
        "    # # data_cutoff_idx: int = 2048"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KmFQwKGcSLr_",
      "metadata": {
        "id": "KmFQwKGcSLr_"
      },
      "source": [
        "### Lets run the training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9MlFpXQFSNdx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MlFpXQFSNdx",
        "outputId": "4f457277-a868-4b22-a97d-f99dc2b97145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resize to max side len: True\n",
            "Loading dataset: tqa\n",
            "Warming up dataloaders...\n",
            "Warmup complete.\n",
            "Loading from backbone weights\n",
            "Successfully loaded google/siglip2-base-patch16-512 weights from safetensors. Model has 86,433,024 parameters.\n",
            "Extending token embeddings from torch.Size([49152, 960]) to torch.Size([49218, 960])\n",
            "Initialized 66 new token embeddings\n",
            "Successfully loaded HuggingFaceTB/SmolLM2-360M-Instruct weights from safetensors. Model has 361,884,480 parameters.\n",
            "nanoVLM initialized with 460,113,984 parameters\n",
            "Training summary: 52 samples, 13 batches/epoch, batch size 4\n",
            "Starting training loop\n"
          ]
        }
      ],
      "source": [
        "# import models.config as config\n",
        "# vlm_cfg = config.VLMConfig()\n",
        "# train_cfg = config.TrainConfig()\n",
        "vlm_cfg = VLMConfig1()\n",
        "train_cfg = TrainConfig1()\n",
        "train(train_cfg, vlm_cfg)\n",
        "\n",
        "# import importlib\n",
        "# import train as train_module\n",
        "# importlib.reload(train_module)\n",
        "\n",
        "# train = train_module.train(train_cfg, vlm_cfg)  # now train() exists"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/nanoVLM/plots/training_dashboard_final.png\")\n"
      ],
      "metadata": {
        "id": "zGpRB3AxIVae"
      },
      "id": "zGpRB3AxIVae",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "id": "NvHrkL0SIrYG"
      },
      "id": "NvHrkL0SIrYG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n"
      ],
      "metadata": {
        "id": "rpf90i3Cf84M"
      },
      "id": "rpf90i3Cf84M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "78d938dd",
      "metadata": {
        "id": "78d938dd"
      },
      "source": [
        "As you can see the model trains, so feel free to play around with the architecture or data! Let us know what you build with it!\n",
        "\n",
        "PS: If you want to test the model, check out generate.py to see how to do inference with it"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86acf5c0d6e741a6891fce588c27b2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_ec525e316c2142d1ac233cb70b59e2ed"
          }
        },
        "b1babd82dee64d12a569c68aa5fcb48e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c608e286fe1a4a3797724a909dd365b9",
            "placeholder": "​",
            "style": "IPY_MODEL_30c9f4edfddb4e8982009868024b07f7",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "e58f56b20c5b41428e99f87e74ed0db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_25c753a85f0e4cafaa3ce23f488ddfdd",
            "placeholder": "​",
            "style": "IPY_MODEL_fe5d0be9f46c4c8f99c2ccbed64b5868",
            "value": ""
          }
        },
        "70948fe6d5f749708401f1b7a93aece3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_e95f40bd09774f2f9e60c9791f37e3df",
            "style": "IPY_MODEL_96fe816cfca54de7a240c31cc3900ca0",
            "value": true
          }
        },
        "1ba21cc831fb42529e59141354da52a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_0694f0bd52c641d08e3b8c1085821245",
            "style": "IPY_MODEL_c20a589cd133478ba9cb838b550bfc14",
            "tooltip": ""
          }
        },
        "8c996cb60d4b4af08354cf7614cc0c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ee2c4891d704c08a9776b7344f6c148",
            "placeholder": "​",
            "style": "IPY_MODEL_fb44e5f2903843b0a76ce236d783a603",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "ec525e316c2142d1ac233cb70b59e2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c608e286fe1a4a3797724a909dd365b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30c9f4edfddb4e8982009868024b07f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25c753a85f0e4cafaa3ce23f488ddfdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe5d0be9f46c4c8f99c2ccbed64b5868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e95f40bd09774f2f9e60c9791f37e3df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96fe816cfca54de7a240c31cc3900ca0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0694f0bd52c641d08e3b8c1085821245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c20a589cd133478ba9cb838b550bfc14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "8ee2c4891d704c08a9776b7344f6c148": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb44e5f2903843b0a76ce236d783a603": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c704b0888c64ce8ac2dd387187c7e26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bb5dc2d759f49d6abc855e97fa4a9e9",
            "placeholder": "​",
            "style": "IPY_MODEL_dab181be12644606a299592a73f26ca2",
            "value": "Connecting..."
          }
        },
        "1bb5dc2d759f49d6abc855e97fa4a9e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dab181be12644606a299592a73f26ca2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}